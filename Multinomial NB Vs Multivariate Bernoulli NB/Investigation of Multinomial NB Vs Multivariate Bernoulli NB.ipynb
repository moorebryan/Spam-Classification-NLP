{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If Wordcloud is not already installed on your system please enter these commands in the terminal:\n",
    "1. git clone https://github.com/amueller/word_cloud.git\n",
    "2. cd word_cloud\n",
    "3. pip install ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now import all modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/bryan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/bryan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "import seaborn as sns\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer, LancasterStemmer\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_curve, precision_recall_curve, classification_report\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# The below suppresses all warnings in the notebook\n",
    "# Only leave this uncommented for display purposes\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 20 categories in the loaded dataset.<br>\n",
    "We will only load the following 4 categories for this to be a simpler example:\n",
    "- alt.atheism\n",
    "- soc.religion.christian\n",
    "- comp.graphics\n",
    "- sci.med"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these cagegories we load in the train and test subsets, shuffling while loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'soc.religion.christian','comp.graphics', 'sci.med']\n",
    "\n",
    "train_data = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42)\n",
    "X_train = train_data.data\n",
    "y_train = train_data.target\n",
    "\n",
    "\n",
    "test_data = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)\n",
    "X_test = test_data.data\n",
    "y_test = test_data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First take a look at what has been loaded and examine a single document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Names:  ['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']\n",
      "\n",
      "Number of Training Examples:  2257\n",
      "Number of Training Labels:  2257\n",
      "Number of Test Examples:  1502\n",
      "Number of Test Labels:  1502\n",
      "\n",
      "Print a Random Document:\n",
      "\n",
      "From: sd345@city.ac.uk (Michael Collier)\n",
      "Subject: Converting images to HP LaserJet III?\n",
      "Nntp-Posting-Host: hampton\n",
      "Organization: The City University\n",
      "Lines: 14\n",
      "\n",
      "Does anyone know of a good way (standard PC application/PD utility) to\n",
      "convert tif/img/tga files into LaserJet III format.  We would also like to\n",
      "do the same, converting to HPGL (HP plotter) files.\n",
      "\n",
      "Please email any response.\n",
      "\n",
      "Is this the correct group?\n",
      "\n",
      "Thanks in advance.  Michael.\n",
      "-- \n",
      "Michael Collier (Programmer)                 The Computer Unit,\n",
      "Email: M.P.Collier@uk.ac.city                The City University,\n",
      "Tel: 071 477-8000 x3769                      London,\n",
      "Fax: 071 477-8565                            EC1V 0HB.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Target Names: \", train_data.target_names)\n",
    "\n",
    "print(\"\\nNumber of Training Examples: \", len(X_train))\n",
    "print(\"Number of Training Labels: \", len(y_train))\n",
    "\n",
    "print(\"Number of Test Examples: \",len(X_test))\n",
    "print(\"Number of Test Labels: \", len(y_test))\n",
    "\n",
    "\n",
    "print(\"\\nPrint a Random Document:\\n\")\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a dataframe from the data and count how many of each category we have.<br>\n",
    "The code for classifications is as given below:<br>\n",
    "0 = 'alt.atheism', 1 = 'soc.religion.christian',2 = 'comp.graphics', 3 = 'sci.med'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3    599\n",
      "2    594\n",
      "1    584\n",
      "0    480\n",
      "Name: 0, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df_full=pd.DataFrame(train_data.target)\n",
    "\n",
    "# Count up number of times each class occurs\n",
    "label_counts = df_full.iloc[:,-1].value_counts()\n",
    "\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a histogram of the number of messages corresponding to each category in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (12,6))\n",
    "sns.barplot(label_counts.index, label_counts.values, alpha = 0.9)\n",
    "\n",
    "plt.xticks(rotation = 'vertical')\n",
    "plt.xlabel('Class Number', fontsize =12)\n",
    "plt.ylabel('Counts', fontsize = 12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create the train and test pandas dataframes.<br>\n",
    "We want them to be dataframes as the next step involves normalization, which is simpler if they are in dataframe format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text_raw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: sd345@city.ac.uk (Michael Collier)\\nSubj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: ani@ms.uky.edu (Aniruddha B. Deglurkar)\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: djohnson@cs.ucsd.edu (Darin Johnson)\\nSu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: s0612596@let.rug.nl (M.M. Zwart)\\nSubjec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: stanly@grok11.columbiasc.ncr.com (stanly...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            text_raw\n",
       "0  From: sd345@city.ac.uk (Michael Collier)\\nSubj...\n",
       "1  From: ani@ms.uky.edu (Aniruddha B. Deglurkar)\\...\n",
       "2  From: djohnson@cs.ucsd.edu (Darin Johnson)\\nSu...\n",
       "3  From: s0612596@let.rug.nl (M.M. Zwart)\\nSubjec...\n",
       "4  From: stanly@grok11.columbiasc.ncr.com (stanly..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I first move the training and testing feature matrices into pandas dataframes\n",
    "df_xTrain = pd.DataFrame()\n",
    "df_xTest = pd.DataFrame()\n",
    "\n",
    "df_xTrain['text_raw'] = X_train\n",
    "df_xTest['text_raw'] = X_test\n",
    "df_xTrain.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we apply Stemming or Lemmatizing to each word in every message.<br>\n",
    "These results are added as separate columns within the same test and train dataframes we created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: ani@ms.uky.edu (Aniruddha B. Deglurkar)\n",
      "Subject: help: Splitting a trimming region along a mesh \n",
      "Organization: University Of Kentucky, Dept. of Math Sciences\n",
      "Lines: 28\n",
      "\n",
      "\n",
      "\n",
      "\tHi,\n",
      "\n",
      "\tI have a problem, I hope some of the 'gurus' can help me solve.\n",
      "\n",
      "\tBackground of the problem:\n",
      "\tI have a rectangular mesh in the uv domain, i.e  the mesh is a \n",
      "\tmapping of a 3d Bezier patch into 2d. The area in this domain\n",
      "\twhich is inside a trimming loop had to be rendered. The trimming\n",
      "\tloop is a set of 2d Bezier curve segments.\n",
      "\tFor the sake of notation: the mesh is made up of cells.\n",
      "\n",
      "\tMy problem is this :\n",
      "\tThe trimming area has to be split up into individual smaller\n",
      "\tcells bounded by the trimming curve segments. If a cell\n",
      "\tis wholly inside the area...then it is output as a whole ,\n",
      "\telse it is trivially rejected. \n",
      "\n",
      "\tDoes any body know how thiss can be done, or is there any algo. \n",
      "\tsomewhere for doing this.\n",
      "\n",
      "\tAny help would be appreciated.\n",
      "\n",
      "\tThanks, \n",
      "\tAni.\n",
      "-- \n",
      "To get irritated is human, to stay cool, divine.\n",
      "\n",
      "\n",
      "Lemmatized Email:\n",
      "\n",
      "From : ani @ ms.uky.edu ( Aniruddha B. Deglurkar ) Subject : help : Splitting a trimming region along a mesh Organization : University Of Kentucky , Dept . of Math Sciences Lines : 28 Hi , I have a problem , I hope some of the 'gurus ' can help me solve . Background of the problem : I have a rectangular mesh in the uv domain , i.e the mesh is a mapping of a 3d Bezier patch into 2d . The area in this domain which is inside a trimming loop had to be rendered . The trimming loop is a set of 2d Bezier curve segment . For the sake of notation : the mesh is made up of cell . My problem is this : The trimming area ha to be split up into individual smaller cell bounded by the trimming curve segment . If a cell is wholly inside the area ... then it is output a a whole , else it is trivially rejected . Does any body know how thiss can be done , or is there any algo . somewhere for doing this . Any help would be appreciated . Thanks , Ani . -- To get irritated is human , to stay cool , divine .\n",
      "\n",
      " Porter Stemmed Email:\n",
      "\n",
      "from : ani @ ms.uky.edu ( aniruddha B. deglurkar ) subject : help : split a trim region along a mesh organ : univers Of kentucki , dept . of math scienc line : 28 Hi , I have a problem , I hope some of the 'guru ' can help me solv . background of the problem : I have a rectangular mesh in the uv domain , i.e the mesh is a map of a 3d bezier patch into 2d . the area in thi domain which is insid a trim loop had to be render . the trim loop is a set of 2d bezier curv segment . for the sake of notat : the mesh is made up of cell . My problem is thi : the trim area ha to be split up into individu smaller cell bound by the trim curv segment . If a cell is wholli insid the area ... then it is output as a whole , els it is trivial reject . doe ani bodi know how thiss can be done , or is there ani algo . somewher for do thi . ani help would be appreci . thank , ani . -- To get irrit is human , to stay cool , divin .\n",
      "\n",
      " Lancaster Stemmed Email:\n",
      "\n",
      "from : an @ ms.uky.edu ( aniruddh b. deglurk ) subject : help : splitting a trim reg along a mesh org : univers of kentucky , dept . of math sci lin : 28 hi , i hav a problem , i hop som of the 'gurus ' can help me solv . background of the problem : i hav a rectangul mesh in the uv domain , i.e the mesh is a map of a 3d bezy patch into 2d . the are in thi domain which is insid a trim loop had to be rend . the trim loop is a set of 2d bezy curv seg . for the sak of not : the mesh is mad up of cel . my problem is thi : the trim are has to be split up into individ smal cel bound by the trim curv seg . if a cel is whol insid the are ... then it is output as a whol , els it is triv reject . doe any body know how thiss can be don , or is ther any algo . somewh for doing thi . any help would be apprecy . thank , an . -- to get irrit is hum , to stay cool , divin .\n"
     ]
    }
   ],
   "source": [
    "# Use a Lemmatizer and see the effect\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "df_xTrain['text_lemmatized'] = df_xTrain['text_raw'].map(lambda text: ' '.join(lemmatizer.lemmatize(w) for w in nltk.word_tokenize(text)))\n",
    "df_xTest['text_lemmatized'] = df_xTest['text_raw'].map(lambda text: ' '.join(lemmatizer.lemmatize(w) for w in nltk.word_tokenize(text)))\n",
    "\n",
    "# Use a Porter Stemmer and see the effect\n",
    "stemmer = PorterStemmer()\n",
    "df_xTrain['text_porter_stemmed'] = df_xTrain['text_raw'].map(lambda text: ' '.join(stemmer.stem(w) for w in nltk.word_tokenize(text)))\n",
    "df_xTest['text_porter_stemmed'] = df_xTest['text_raw'].map(lambda text: ' '.join(stemmer.stem(w) for w in nltk.word_tokenize(text)))\n",
    "\n",
    "# Use a Lancaster Stemmer and see the effect\n",
    "stemmer = LancasterStemmer()\n",
    "df_xTrain['text_lancaster_stemmed'] = df_xTrain['text_raw'].map(lambda text: ' '.join(stemmer.stem(w) for w in nltk.word_tokenize(text)))\n",
    "df_xTest['text_lancaster_stemmed'] = df_xTest['text_raw'].map(lambda text: ' '.join(stemmer.stem(w) for w in nltk.word_tokenize(text)))\n",
    "\n",
    "# Observe a random document to see the effect of each\n",
    "doc_num = 1\n",
    "print(df_xTrain['text_raw'][doc_num])\n",
    "print(\"\\nLemmatized Email:\\n\")\n",
    "print(df_xTrain['text_lemmatized'][doc_num])\n",
    "print(\"\\n Porter Stemmed Email:\\n\")\n",
    "print(df_xTrain['text_porter_stemmed'][doc_num])\n",
    "print(\"\\n Lancaster Stemmed Email:\\n\")\n",
    "print(df_xTrain['text_lancaster_stemmed'][doc_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create the pipeline we will use for performing hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_clf_multinomialNB = Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('clf', MultinomialNB()),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the parameters over which you will perform the hyperparameter optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "    'vect__stop_words': ['english', None],\n",
    "    'vect__binary': [True, False],\n",
    "    'clf__alpha': [0.1, 1.0, 1.5, 1.8],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the gridsearch we will be applying below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As using f1 score works only for binary class targets we instead implement f1_micro\n",
    "clf_multinomial_cv = GridSearchCV(text_clf_multinomialNB, param_grid, scoring='f1_micro', cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For below we choose to define our feature matrix depending on no normalizing (raw) or by choosing one of the normalizing approaches.<br>\n",
    "Note once X_train_proc and X_test_proc are defined here they are used below, so uncomment only the normalizing approach (or lack thereof) you want to apply below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_proc = df_xTrain['text_raw'] # Use raw data\n",
    "X_test_proc = df_xTest['text_raw'] # Use raw data\n",
    "\n",
    "# X_train_proc = df_xTrain['text_lemmatized'] # Use lemmatized data\n",
    "# X_test_proc = df_xTest['text_lemmatized'] # Use lemmatized data\n",
    "\n",
    "# X_train_proc = df_xTrain['text_porter_stemmed'] # Use porter stemmed data\n",
    "# X_test_proc = df_xTest['text_porter_stemmed'] # Use porter stemmed data\n",
    "\n",
    "# X_train_proc = df_xTrain['text_lancaster_stemmed'] # Use lancaster stemmed data\n",
    "# X_test_proc = df_xTest['text_lancaster_stemmed'] # Use lancaster stemmed data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now apply hyperparameter optimization using the normalization approach chosen above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Score: 0.981391\n",
      "\n",
      "Optimal Hyperparameter Values: \n",
      "clf__alpha: 0.1\n",
      "vect__binary: True\n",
      "vect__ngram_range: (1, 2)\n",
      "vect__stop_words: 'english'\n"
     ]
    }
   ],
   "source": [
    "clf_multinomial_cv = clf_multinomial_cv.fit(X_train_proc, y_train)\n",
    "\n",
    "\n",
    "print(\"\\nBest Score: %f\" % clf_multinomial_cv.best_score_)\n",
    "\n",
    "print(\"\\nOptimal Hyperparameter Values: \")\n",
    "\n",
    "for param_name in sorted(param_grid.keys()):\n",
    "    print(\"%s: %r\" % (param_name, clf_multinomial_cv.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below I have recorded the optimal parameters found for all normalization approaches tested above<br>\n",
    "Uncomment the parameters corresponding to the normalization approach you have chosen to apply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this box uncomment the optimal hyperparameters for the lemmatization/stemming performed\n",
    "\n",
    "# Below was done for raw data\n",
    "stop_words_opt = 'english'\n",
    "ngram_range_opt = (1, 2)\n",
    "binary_opt = True\n",
    "clf_alpha_opt = 0.1\n",
    "\n",
    "# Below was done for lemmatized data\n",
    "# stop_words_opt = 'english'\n",
    "# ngram_range_opt = (1, 2)\n",
    "# binary_opt = True\n",
    "# clf_alpha_opt = 0.1\n",
    "\n",
    "# Below was done for Porter Stemmed Data\n",
    "# stop_words_opt = 'english'\n",
    "# ngram_range_opt = (1, 2)\n",
    "# binary_opt = True\n",
    "# clf_alpha_opt = 0.1\n",
    "\n",
    "# Below was done for Lancaster Stemmed Data\n",
    "# stop_words_opt = None\n",
    "# ngram_range_opt = (1, 2)\n",
    "# binary_opt = True\n",
    "# clf_alpha_opt = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I now create the pipeline I will use for evaluating the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "multinomialNB_clf = Pipeline([\n",
    "        ('vect', CountVectorizer(stop_words=stop_words_opt, ngram_range=ngram_range_opt, binary=binary_opt)),\n",
    "        ('clf', MultinomialNB(alpha=clf_alpha_opt)),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now fit the training data to this pipeline, meaning all chosen parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words='english',\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)), ('clf', MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multinomialNB_clf.fit(X_train_proc, y_train)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now apply this model to the test data and see how effective it is at categorizing messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Confusion Matrix for Multinomial NB:\n",
      "[[297   3   6  13]\n",
      " [  4 375   9   1]\n",
      " [  5  19 360  12]\n",
      " [  4   2   7 385]]\n",
      "\n",
      "Test Precision for Multinomial NB = 0.943409\n",
      "Test Recall for Multinomial NB = 0.943409\n",
      "Test F1 Score for Multinomial NB = 0.943409\n",
      "\n",
      "Classification Report for Multinomial NB:\n",
      "                            precision    recall  f1-score   support\n",
      "\n",
      "           alt.atheism (0)       0.96      0.93      0.94       319\n",
      "soc.religion.christian (1)       0.94      0.96      0.95       389\n",
      "         comp.graphics (2)       0.94      0.91      0.93       396\n",
      "               sci.med (3)       0.94      0.97      0.95       398\n",
      "\n",
      "                 micro avg       0.94      0.94      0.94      1502\n",
      "                 macro avg       0.94      0.94      0.94      1502\n",
      "              weighted avg       0.94      0.94      0.94      1502\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make prediction\n",
    "y_test_predicted = multinomialNB_clf.predict(X_test_proc)\n",
    "\n",
    "print(\"\\nTest Confusion Matrix for Multinomial NB:\")\n",
    "print(confusion_matrix(y_test, y_test_predicted))\n",
    "\n",
    "precision_test = precision_score(y_test, y_test_predicted,average='micro') \n",
    "print(\"\\nTest Precision for Multinomial NB = %f\" % precision_test)\n",
    "\n",
    "recall_test = recall_score(y_test, y_test_predicted,average='micro')\n",
    "print(\"Test Recall for Multinomial NB = %f\" % recall_test)\n",
    "\n",
    "\n",
    "f1_test = f1_score(y_test, y_test_predicted,average='micro')\n",
    "print(\"Test F1 Score for Multinomial NB = %f\" % f1_test)\n",
    "\n",
    "print(\"\\nClassification Report for Multinomial NB:\")\n",
    "print(classification_report(y_test, y_test_predicted, target_names = ['alt.atheism (0)', 'soc.religion.christian (1)','comp.graphics (2)', 'sci.med (3)']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now test the same model, but now with TFIDF as well.<br>\n",
    "Below is the pipeline used for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tfidf_clf_multinomialNB = Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', MultinomialNB()),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use set up this pipeline with the optimal parameters found above.<br>\n",
    "Note that technically we should apply hyperparameter optimization again, but I have not tested this yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "multinomialNB_clf_tfidf = Pipeline([\n",
    "        ('vect', CountVectorizer(stop_words=stop_words_opt, ngram_range=ngram_range_opt, binary=binary_opt)),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', MultinomialNB(alpha=clf_alpha_opt)),\n",
    "    ])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now fit this to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words='english',\n",
       "        s...inear_tf=False, use_idf=True)), ('clf', MultinomialNB(alpha=0.1, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multinomialNB_clf_tfidf.fit(X_train_proc, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use this model to make predictions on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Confusion Matrix for Multinomial NB With TFIDF:\n",
      "[[276   3  11  29]\n",
      " [  1 372   8   8]\n",
      " [  4  22 353  17]\n",
      " [  4   3   6 385]]\n",
      "\n",
      "Test Precision for Multinomial NB With TFIDF = 0.922770\n",
      "Test Recall for Multinomial NB With TFIDF = 0.922770\n",
      "Test F1 Score for Multinomial NB With TFIDF = 0.922770\n",
      "\n",
      "Classification Report for Multinomial NB With TFIDF:\n",
      "                            precision    recall  f1-score   support\n",
      "\n",
      "           alt.atheism (0)       0.97      0.87      0.91       319\n",
      "soc.religion.christian (1)       0.93      0.96      0.94       389\n",
      "         comp.graphics (2)       0.93      0.89      0.91       396\n",
      "               sci.med (3)       0.88      0.97      0.92       398\n",
      "\n",
      "                 micro avg       0.92      0.92      0.92      1502\n",
      "                 macro avg       0.93      0.92      0.92      1502\n",
      "              weighted avg       0.93      0.92      0.92      1502\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make prediction\n",
    "y_test_predicted = multinomialNB_clf_tfidf.predict(X_test_proc)\n",
    "\n",
    "print(\"\\nTest Confusion Matrix for Multinomial NB With TFIDF:\")\n",
    "print(confusion_matrix(y_test, y_test_predicted))\n",
    "\n",
    "precision_test = precision_score(y_test, y_test_predicted,average='micro') \n",
    "print(\"\\nTest Precision for Multinomial NB With TFIDF = %f\" % precision_test)\n",
    "\n",
    "recall_test = recall_score(y_test, y_test_predicted,average='micro')\n",
    "print(\"Test Recall for Multinomial NB With TFIDF = %f\" % recall_test)\n",
    "\n",
    "\n",
    "f1_test = f1_score(y_test, y_test_predicted,average='micro')\n",
    "print(\"Test F1 Score for Multinomial NB With TFIDF = %f\" % f1_test)\n",
    "\n",
    "print(\"\\nClassification Report for Multinomial NB With TFIDF:\")\n",
    "print(classification_report(y_test, y_test_predicted, target_names = ['alt.atheism (0)', 'soc.religion.christian (1)','comp.graphics (2)', 'sci.med (3)']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that as we did not perform cross-validation after including TFIDF this could be one possible explanation for the drop in precion and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now implement Multivariate Bernoulli Naive Bayes<br>\n",
    "We first build a pipeline using the same optimal parameters found above.<br>\n",
    "Note that technically we should apply hyperparameter optimization again, but I have not tested this yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bernoulliNB_clf = Pipeline([\n",
    "        ('vect', CountVectorizer(stop_words='english', ngram_range=(1, 2), binary=True)),\n",
    "        ('clf', BernoulliNB(alpha=0.1)),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now fit this to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words='english',\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)), ('clf', BernoulliNB(alpha=0.1, binarize=0.0, class_prior=None, fit_prior=True))])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bernoulliNB_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now evaluate this Multivariate Bernoulli Naive Bayes model on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Confusion Matrix for Multivariate Bernoulli NB:\n",
      "[[288  18   5   8]\n",
      " [  2 385   2   0]\n",
      " [  3 107 284   2]\n",
      " [  3  36   1 358]]\n",
      "\n",
      "Test Precision for Multivariate Bernoulli NB = 0.875499\n",
      "Test Recall for Multivariate Bernoulli NB = 0.875499\n",
      "Test F1 Score for Multivariate Bernoulli NB = 0.875499\n",
      "\n",
      "Classification Report for Multivariate Bernoulli NB:\n",
      "                            precision    recall  f1-score   support\n",
      "\n",
      "           alt.atheism (0)       0.97      0.90      0.94       319\n",
      "soc.religion.christian (1)       0.71      0.99      0.82       389\n",
      "         comp.graphics (2)       0.97      0.72      0.83       396\n",
      "               sci.med (3)       0.97      0.90      0.93       398\n",
      "\n",
      "                 micro avg       0.88      0.88      0.88      1502\n",
      "                 macro avg       0.91      0.88      0.88      1502\n",
      "              weighted avg       0.90      0.88      0.88      1502\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Make prediction\n",
    "y_test_predicted = bernoulliNB_clf.predict(X_test_proc)\n",
    "\n",
    "print(\"\\nTest Confusion Matrix for Multivariate Bernoulli NB:\")\n",
    "print(confusion_matrix(y_test, y_test_predicted))\n",
    "\n",
    "precision_test = precision_score(y_test, y_test_predicted,average='micro') \n",
    "print(\"\\nTest Precision for Multivariate Bernoulli NB = %f\" % precision_test)\n",
    "\n",
    "recall_test = recall_score(y_test, y_test_predicted,average='micro')\n",
    "print(\"Test Recall for Multivariate Bernoulli NB = %f\" % recall_test)\n",
    "\n",
    "\n",
    "f1_test = f1_score(y_test, y_test_predicted,average='micro')\n",
    "print(\"Test F1 Score for Multivariate Bernoulli NB = %f\" % f1_test)\n",
    "\n",
    "print(\"\\nClassification Report for Multivariate Bernoulli NB:\")\n",
    "print(classification_report(y_test, y_test_predicted, target_names = ['alt.atheism (0)', 'soc.religion.christian (1)','comp.graphics (2)', 'sci.med (3)']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One issue which I have with the approach taken in this notebook is that we perform cross-validation only on the model which included multinomial Naive-Bayes Classification. We never re-performed cross-validation after including TFIDF as well, or after implimenting the multivariate Bernouli model. This thus could skew some of the results. With that in mind our summary of results are below:\n",
    "\n",
    "\n",
    "Below is a summary of the weighted average for all error metrics using no Lemmatization or Stemming:<br>\n",
    "Multinomial Naive Bayes: Precision=0.94 .  Recall=0.94 .  F1-Score=0.94<br>\n",
    "MultiNomial Naive Bayes with TFIDF: Precision=0.93 .  Recall=0.92 .  F1-Score=0.92<br>\n",
    "Multivariate Bernoulli: Precision=0.90 .  Recall=0.88 .  F1-Score=0.88<br>\n",
    "\n",
    "Below is a summary of the weighted average for all error metrics using Lemmatization:<br>\n",
    "Multinomial Naive Bayes: Precision=0.94 .  Recall=0.94 .  F1-Score=0.94<br>\n",
    "MultiNomial Naive Bayes with TFIDF: Precision=0.93 .  Recall=0.92 .  F1-Score=0.92<br>\n",
    "Multivariate Bernoulli: Precision=0.90 .  Recall=0.87 .  F1-Score=0.87<br>\n",
    "\n",
    "Below is a summary of the weighted average for all error metrics using Porter Stemming:<br>\n",
    "Multinomial Naive Bayes: Precision=0.94 .  Recall=0.94 .  F1-Score=0.94<br>\n",
    "MultiNomial Naive Bayes with TFIDF: Precision=0.92 .  Recall=0.92 .  F1-Score=0.92<br>\n",
    "Multivariate Bernoulli: Precision=0.87 .  Recall=0.80 .  F1-Score=0.81<br>\n",
    "\n",
    "Below is a summary of the weighted average for all error metrics using Lancaster Stemming:<br>\n",
    "Multinomial Naive Bayes: Precision=0.93 .  Recall=0.93 .  F1-Score=0.93<br>\n",
    "MultiNomial Naive Bayes with TFIDF: Precision=0.90 .  Recall=0.90 .  F1-Score=0.90<br>\n",
    "Multivariate Bernoulli: Precision=0.85 .  Recall=0.73 .  F1-Score=0.73<br>\n",
    "\n",
    "\n",
    "From the above results it appears that regardless of the pre-processing (lemitization or stemming) performed that the Multinomial Naive Bayes Classification provides the optimal results. As the Multinomial Naive Bayes Classification with TFIDF performs slightly worse it appears that weighting particular words by frequency actually hurts the predictive ability, at least for categorizing for these categories. That multivariate bernouli performs worst indicates that for categorizing these type of messages the frequency of the words (how often they are used per message) does contain important predictive information.<br>\n",
    "\n",
    "For assessing the impact of Lemmatization and Stemming we tested each separately. Due to lack of time we did not test chained combinations, where for example we could have performed lemmatization and then stemming. Performing each separately showed that, surprisingly, there was not a significant improvement in performing stemming or lemitization. In fact, lemmatization provided no improvement for Multinomial Naive Bayes or Multinomial Naive Bayes with TFIDF but actually decreased the predictive ability for Multivariate Bernoulli. Stemming performed even worse. For both types of stemming assessed these caused a decrease in predictive ability for every Naive Bayes model used. Thus, we conclude that for this classification it is probably best to not utilize lemmatization or stemming.<br>\n",
    "\n",
    "In conlcusion, the above results lead us to recommend the following model for performing this classification. No normalization should be used, and for the classification model we recommend using Multinomial Naive Bayes with the following hyperparameters:<br>\n",
    "stop_words = 'english'<br>\n",
    "ngram_range = (1, 2)<br>\n",
    "binary = True<br>\n",
    "clf_alpha = 0.1<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
